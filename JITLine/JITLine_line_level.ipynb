{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "from my_util import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time, pickle, math, warnings, os, operator\n",
    "from imblearn.over_sampling import SMOTE\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "import dill\n",
    "\n",
    "data_path = './text_metric_data/'\n",
    "model_path = './text_metric_model/'\n",
    "\n",
    "projects = ['openstack','qt']\n",
    "\n",
    "remove_python_common_tokens = True\n",
    "\n",
    "    \n",
    "top_k_tokens = np.arange(10,201,10)\n",
    "agg_methods = ['avg','median','sum']\n",
    "max_str_len_list = 100\n",
    "\n",
    "# since we don't want to use commit metrics in LIME\n",
    "commit_metrics = ['la','la','ld', 'ld', 'nf','nd_y', 'nd', 'ns','ent', 'ent', 'nrev', 'rtime', 'hcmt', 'self', 'ndev',\n",
    "                          'age', 'age', 'nuc', 'app_y', 'aexp', 'rexp', 'arexp', 'rrexp', 'asexp', 'rsexp', 'asawr', 'rsawr']\n",
    "\n",
    "line_score_df_col_name = ['line_num', 'code_line', 'total_tokens'] + ['token'+str(i) for i in range(1,max_str_len_list+1)] + [agg+'-top-'+str(k)+'-tokens' for agg in agg_methods for k in top_k_tokens] + [agg+'-all-tokens' for agg in agg_methods]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps for line-level\n",
    "\n",
    "1. load prediction result, line-level dataset, train/test data\n",
    "2. load RF model\n",
    "3. resampling dataset (like in commit-level experiment)\n",
    "4. train LIME model\n",
    "5. get correctly predicted result\n",
    "6. for each correctly predicted result\n",
    "    - get explanation from LIME\n",
    "    - get line score (mean, median, sum)\n",
    "    - put commit_id, code line, line score, label to dataframe\n",
    "7. combine result in 6. as a single dataframe\n",
    "\n",
    "note: no need to use differential evolution algorithm to find the best k_neighbors since it is known from training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combined_features(code_commit, commit_id, label, metrics_df, count_vect, mode = 'train'):\n",
    "    \n",
    "    if mode not in ['train','test']:\n",
    "        print('wrong mode')\n",
    "        return\n",
    "    \n",
    "    code_df = pd.DataFrame()\n",
    "    code_df['commit_id'] = commit_id\n",
    "    code_df['code'] = code_commit\n",
    "    code_df['label'] = label\n",
    "    \n",
    "    code_df = code_df.sort_values(by='commit_id')\n",
    "    \n",
    "    metrics_df = metrics_df.sort_values(by='commit_id')\n",
    "#     metrics_df = metrics_df.drop('commit_id',axis=1)\n",
    "    \n",
    "    code_change_arr = count_vect.transform(code_df['code']).astype(np.int16).toarray()\n",
    "    \n",
    "    if mode == 'train':\n",
    "        metrics_df = metrics_df.drop('commit_id',axis=1)\n",
    "        metrics_df_arr = metrics_df.to_numpy(dtype=np.float32)\n",
    "        final_features = np.concatenate((code_change_arr,metrics_df_arr),axis=1)\n",
    "        col_names = list(count_vect.get_feature_names())+list(metrics_df.columns)\n",
    "        return final_features, col_names, list(code_df['label'])\n",
    "    elif mode == 'test':\n",
    "        code_features = pd.DataFrame(code_change_arr, columns=count_vect.get_feature_names())\n",
    "        code_features['commit_id'] = list(code_df['commit_id'])\n",
    "\n",
    "        metrics_df = metrics_df.set_index('commit_id')\n",
    "        code_features = code_features.set_index('commit_id')\n",
    "        final_features = pd.concat([code_features, metrics_df],axis=1)\n",
    "        \n",
    "        return final_features, list(code_df['commit_id']), list(code_df['label'])\n",
    "\n",
    "def get_LIME_explainer(proj_name, train_feature, feature_names):\n",
    "    LIME_explainer_path = './text_metric_model/'+proj_name+'_LIME_RF_DE_SMOTE_min_df_3.pkl'\n",
    "    class_names = ['not defective', 'defective'] # this is fine...\n",
    "    if not os.path.exists(LIME_explainer_path):\n",
    "        start = time.time()\n",
    "        # get features in train_df here\n",
    "        print('start training LIME explainer')\n",
    "\n",
    "        explainer = LimeTabularExplainer(train_feature, \n",
    "                                         feature_names=feature_names, \n",
    "                                         class_names=class_names, discretize_continuous=False, random_state=42)\n",
    "        dill.dump(explainer, open(LIME_explainer_path, 'wb'))\n",
    "        print('finish training LIME explainer in',time.time()-start, 'secs')\n",
    "\n",
    "    else:\n",
    "        explainer = dill.load(open(LIME_explainer_path, 'rb'))\n",
    "    \n",
    "    return explainer\n",
    "\n",
    "def eval_with_LIME(proj_name, clf, explainer, test_features):\n",
    "    \n",
    "    def preprocess_feature_from_explainer(exp):\n",
    "        features_val = exp.as_list(label=1)\n",
    "        new_features_val = [tup for tup in features_val if float(tup[1]) > 0] # only score > 0 that indicates buggy token\n",
    "\n",
    "        feature_dict = {re.sub('\\s.*','',val[0]):val[1] for val in new_features_val}\n",
    "\n",
    "        sorted_feature_dict = sorted(feature_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "        sorted_feature_dict = {tup[0]:tup[1] for tup in sorted_feature_dict if tup[0] not in commit_metrics}\n",
    "        tokens_list = list(sorted_feature_dict.keys())\n",
    "\n",
    "        return sorted_feature_dict, tokens_list\n",
    "    \n",
    "    def add_agg_scr_to_list(line_stuff, scr_list):\n",
    "        if len(scr_list) < 1:\n",
    "            scr_list.append(0)\n",
    "\n",
    "        line_stuff.append(np.mean(scr_list))\n",
    "        line_stuff.append(np.median(scr_list))\n",
    "        line_stuff.append(np.sum(scr_list))\n",
    "\n",
    "    all_buggy_line_result_df = []  \n",
    "    \n",
    "    prediction_result = pd.read_csv(data_path+proj_name+'_RF_DE_SMOTE_min_df_3_prediction_result.csv')\n",
    "    line_level_df = pd.read_csv('./dataset/'+proj_name+'_complete_buggy_line_level.csv',sep='\\t')\n",
    "\n",
    "    correctly_predicted_commit = list(prediction_result[(prediction_result['pred']==1) &\n",
    "                                                (prediction_result['actual']==1)]['test_commit'])\n",
    "\n",
    "    all_hash = list(line_level_df['commit_hash'])\n",
    "     \n",
    "    for commit in correctly_predicted_commit:\n",
    "        code_change_from_line_level_df = list(line_level_df[line_level_df['commit_hash']==commit]['code_change'])\n",
    "        line_level_label = list(line_level_df[line_level_df['commit_hash']==commit]['is_buggy_line'])\n",
    "\n",
    "        line_score_df = pd.DataFrame(columns = line_score_df_col_name)\n",
    "        line_score_df['line_num'] = np.arange(0,len(code_change_from_line_level_df))\n",
    "        line_score_df = line_score_df.set_index('line_num')\n",
    "\n",
    "        exp = explainer.explain_instance(test_features.loc[commit], clf.predict_proba, \n",
    "                                         num_features=len(test_features.columns), top_labels=1)\n",
    "\n",
    "        sorted_feature_score_dict, tokens_list = preprocess_feature_from_explainer(exp)\n",
    "\n",
    "        for line_num, line in enumerate(code_change_from_line_level_df): # for each line (sadly this loop is needed...)\n",
    "            line_stuff = []\n",
    "            line_score_list = np.zeros(100) # this is needed to store result in dataframe\n",
    "            token_list = line.split()[:100]\n",
    "            line_stuff.append(line)\n",
    "            line_stuff.append(len(token_list))\n",
    "\n",
    "            for tok_idx, tok in enumerate(token_list):\n",
    "                score = sorted_feature_score_dict.get(tok,0)\n",
    "                line_score_list[tok_idx] = score\n",
    "                \n",
    "            # calculate top-k tokens first then followed by all tokens\n",
    "\n",
    "            line_stuff = line_stuff + list(line_score_list)\n",
    "\n",
    "            for k in top_k_tokens: # for each k in top-k tokens\n",
    "                top_tokens = tokens_list[0:k-1]\n",
    "                top_k_scr_list = []\n",
    "\n",
    "                if len(token_list) < 1:\n",
    "                    top_k_scr_list.append(0)\n",
    "                else:\n",
    "                    for tok in token_list:\n",
    "                        score = 0\n",
    "                        if tok in top_tokens:\n",
    "                            score = sorted_feature_score_dict.get(tok,0)\n",
    "                        top_k_scr_list.append(score)\n",
    "\n",
    "                # agg_methods = ['avg','median','sum','max']\n",
    "                add_agg_scr_to_list(line_stuff, top_k_scr_list) # add to line_stuff\n",
    "\n",
    "            add_agg_scr_to_list(line_stuff, list(line_score_list[:len(token_list)])) # add to line_stuff\n",
    "            line_score_df.loc[line_num] = line_stuff\n",
    "\n",
    "        line_score_df['commit_id'] = [commit]*len(line_level_label)\n",
    "        line_score_df['line_level_label'] = line_level_label\n",
    "\n",
    "        all_buggy_line_result_df.append(line_score_df)\n",
    "        \n",
    "    return all_buggy_line_result_df\n",
    "\n",
    "# 'openstack' or 'qt'\n",
    "def eval_line_level(proj_name):\n",
    "    # load model here\n",
    "    clf = pickle.load(open(model_path+proj_name+'_RF_DE_SMOTE_min_df_3.pkl','rb'))\n",
    "\n",
    "    train_code, train_commit, train_label = prepare_data(proj_name, mode='train',\n",
    "                                                                  remove_python_common_tokens=remove_python_common_tokens)\n",
    "    test_code, test_commit, test_label = prepare_data(proj_name, mode='test',\n",
    "                                                              remove_python_common_tokens=remove_python_common_tokens)\n",
    "\n",
    "    commit_metrics = load_change_metrics_df(proj_name)\n",
    "    #     print(commit_metrics.head())\n",
    "    train_commit_metrics = commit_metrics[commit_metrics['commit_id'].isin(train_commit)]\n",
    "    test_commit_metrics = commit_metrics[commit_metrics['commit_id'].isin(test_commit)]\n",
    "\n",
    "    count_vect = CountVectorizer(min_df=3, ngram_range=(1,1))\n",
    "    count_vect.fit(train_code)\n",
    "\n",
    "    # use train_feature to train LIME\n",
    "    # use test_feature to get\n",
    "    train_feature, col_names, new_train_label = get_combined_features(train_code, train_commit, train_label, train_commit_metrics,count_vect)\n",
    "    test_feature, test_commit_id, new_test_label = get_combined_features(test_code, test_commit, test_label, test_commit_metrics,count_vect, mode = 'test')\n",
    "\n",
    "    print('load data of',proj_name, 'finish') # at least we can load dataframe...\n",
    "    #     print(train_feature.head(100))\n",
    "    \n",
    "    k = 0 # k_neighbors of SMOTE, obtained from DE at commit level\n",
    "    if proj_name == 'openstack':\n",
    "        k = 17\n",
    "    elif proj_name == 'qt':\n",
    "        k = 8\n",
    "    \n",
    "    smote = SMOTE(k_neighbors = k, random_state=42, n_jobs=-1)\n",
    "\n",
    "    train_feature_res, new_train_label_res = smote.fit_resample(train_feature, new_train_label)\n",
    "    print('resample data complete')\n",
    "    \n",
    "    explainer = get_LIME_explainer(proj_name, train_feature_res, col_names)\n",
    "    print('load LIME explainer complete')\n",
    "    \n",
    "    line_level_result = eval_with_LIME(proj_name, clf, explainer, test_feature)\n",
    "    \n",
    "    print('eval line level finish')\n",
    "    return line_level_result\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data of openstack finish\n",
      "resample data complete\n",
      "load LIME explainer complete\n",
      "eval line level finish\n"
     ]
    }
   ],
   "source": [
    "openstack_line_level = eval_line_level('openstack')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data of qt finish\n",
      "resample data complete\n",
      "load LIME explainer complete\n",
      "eval line level finish\n"
     ]
    }
   ],
   "source": [
    "qt_line_level = eval_line_level('qt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(openstack_line_level.head())\n",
    "pd.concat(openstack_line_level).to_csv(data_path+'openstack_line_level_result.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(qt_line_level).to_csv(data_path+'qt_line_level_result.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Oat)",
   "language": "python",
   "name": "env_oat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, roc_auc_score, classification_report\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from new_JIT_defect_prediction_dir.my_util import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_data_dir = './text_metric_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recall_at_k_percent_effort(percent_effort, result_df_arg, real_buggy_commits):\n",
    "    cum_LOC_k_percent = (percent_effort/100)*result_df_arg.iloc[-1]['cum_LOC']\n",
    "    buggy_line_k_percent =  result_df_arg[result_df_arg['cum_LOC'] <= cum_LOC_k_percent]\n",
    "    buggy_commit = buggy_line_k_percent[buggy_line_k_percent['label']==1]\n",
    "    recall_k_percent_effort = len(buggy_commit)/float(len(real_buggy_commits))\n",
    "    \n",
    "    return recall_k_percent_effort\n",
    "\n",
    "def eval_metrics(method, result_df):\n",
    "    \n",
    "    pred = result_df['defective_commit_pred']\n",
    "    y_test = result_df['label']\n",
    "    \n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_test,pred,average='binary') # at threshold = 0.5\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, pred, labels=[0, 1]).ravel()\n",
    "#     rec = tp/(tp+fn)\n",
    "    \n",
    "    FAR = fp/(fp+tn) # false alarm rate\n",
    "    dist_heaven = math.sqrt((pow(1-rec,2)+pow(0-FAR,2))/2.0) # distance to heaven\n",
    "    \n",
    "    auc = roc_auc_score(y_test, result_df['defective_commit_prob'])\n",
    "\n",
    "    result_df['defect_density'] = result_df['defective_commit_prob']/result_df['LOC'] # predicted defect density\n",
    "    result_df['actual_defect_density'] = result_df['label']/result_df['LOC'] #defect density\n",
    "\n",
    "    result_df = result_df.sort_values(by='defect_density',ascending=False)\n",
    "    actual_result_df = result_df.sort_values(by='actual_defect_density',ascending=False)\n",
    "    actual_worst_result_df = result_df.sort_values(by='actual_defect_density',ascending=True)\n",
    "\n",
    "    result_df['cum_LOC'] = result_df['LOC'].cumsum()\n",
    "    actual_result_df['cum_LOC'] = actual_result_df['LOC'].cumsum()\n",
    "    actual_worst_result_df['cum_LOC'] = actual_worst_result_df['LOC'].cumsum()\n",
    "\n",
    "    real_buggy_commits = result_df[result_df['label'] == 1]\n",
    "\n",
    "    label_list = list(result_df['label'])\n",
    "\n",
    "    all_rows = len(label_list)\n",
    "\n",
    "    # find recall (use LOC to find 20% of buggy commit)\n",
    "    cum_LOC_20_percent = 0.2*result_df.iloc[-1]['cum_LOC']\n",
    "    buggy_line_20_percent = result_df[result_df['cum_LOC'] <= cum_LOC_20_percent]\n",
    "    buggy_commit = buggy_line_20_percent[buggy_line_20_percent['label']==1]\n",
    "    recall_20_percent_effort = len(buggy_commit)/float(len(real_buggy_commits))\n",
    "\n",
    "    # find effort @20% LOC recall\n",
    "    buggy_20_percent = real_buggy_commits.head(math.ceil(0.2 * len(real_buggy_commits)))\n",
    "    buggy_20_percent_LOC = buggy_20_percent.iloc[-1]['cum_LOC']\n",
    "    effort_at_20_percent_LOC_recall = int(buggy_20_percent_LOC) / float(result_df.iloc[-1]['cum_LOC'])\n",
    "    \n",
    "    # find P_opt\n",
    "    percent_effort_list = []\n",
    "    predicted_recall_at_percent_effort_list = []\n",
    "    actual_recall_at_percent_effort_list = []\n",
    "    actual_worst_recall_at_percent_effort_list = []\n",
    "    \n",
    "    for percent_effort in np.arange(10,101,10):\n",
    "        predicted_recall_k_percent_effort = get_recall_at_k_percent_effort(percent_effort, result_df, real_buggy_commits)\n",
    "        actual_recall_k_percent_effort = get_recall_at_k_percent_effort(percent_effort, actual_result_df, real_buggy_commits)\n",
    "        actual_worst_recall_k_percent_effort = get_recall_at_k_percent_effort(percent_effort, actual_worst_result_df, real_buggy_commits)\n",
    "        \n",
    "        percent_effort_list.append(percent_effort/100)\n",
    "        \n",
    "        predicted_recall_at_percent_effort_list.append(predicted_recall_k_percent_effort)\n",
    "        actual_recall_at_percent_effort_list.append(actual_recall_k_percent_effort)\n",
    "        actual_worst_recall_at_percent_effort_list.append(actual_worst_recall_k_percent_effort)\n",
    "\n",
    "    p_opt = 1 - ((metrics.auc(percent_effort_list, actual_recall_at_percent_effort_list) - \n",
    "                 metrics.auc(percent_effort_list, predicted_recall_at_percent_effort_list)) /\n",
    "                (metrics.auc(percent_effort_list, actual_recall_at_percent_effort_list) -\n",
    "                metrics.auc(percent_effort_list, actual_worst_recall_at_percent_effort_list)))\n",
    "    \n",
    "    return prec, rec, f1, auc, FAR, dist_heaven, recall_20_percent_effort, effort_at_20_percent_LOC_recall, p_opt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_result(proj_name,sampling_method = 'DE_SMOTE_min_df_3'):\n",
    "    \n",
    "    RF_result = pd.read_csv(RF_data_dir+proj_name+'_RF_'+sampling_method+'_prediction_result.csv')\n",
    "    \n",
    "    RF_result.columns = ['Unnamed', 'defective_commit_prob','defective_commit_pred','label','test_commit'] # for new result\n",
    "\n",
    "    test_combined_code, test_commit, test_label = prepare_data(proj_name, mode='test',use_text=True,\n",
    "                                                          remove_python_common_tokens=False, data_dir='./new_JIT_defect_prediction_dir/dataset/')\n",
    "\n",
    "    RF_LOC = [len(code.splitlines()) for code in test_combined_code]\n",
    "    RF_df = pd.DataFrame()\n",
    "    RF_df['commit_id'] = test_commit\n",
    "    RF_df['LOC'] = RF_LOC\n",
    "\n",
    "    RF_result = pd.merge(RF_df, RF_result,how='inner',left_on = 'commit_id', right_on='test_commit')\n",
    "\n",
    "    prec, rec, f1, auc, FAR, dist_heaven, recall_20_percent_effort, effort_at_20_percent_LOC_recall,p_opt = eval_metrics(method, df)\n",
    "    \n",
    "    \n",
    "    print('Precision: {:.2f}, Recall: {:.2f}, F1: {:.2f}, AUC: {:.2f}, FAR: {:.2f}, d2h: {:.2f}, PCI@20%LOC: {:.2f}, Effort@20%Recall: {:.2f}, POpt: {:.2f}'.format(prec, rec, f1, auc, FAR, dist_heaven, recall_20_percent_effort, effort_at_20_percent_LOC_recall,p_opt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result('openstack')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result('qt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Oat)",
   "language": "python",
   "name": "env_oat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_util import *\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, matthews_corrcoef, precision_recall_fscore_support, balanced_accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import time, pickle, warnings, operator, math, os\n",
    "# import dill\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_data_path = './text_metric_data/'\n",
    "ngram_data_path = './for_ngram_model/'\n",
    "\n",
    "projects = ['openstack','qt']\n",
    "\n",
    "top_k_tokens = np.arange(10,201,10)\n",
    "agg_methods = ['avg','median','sum']\n",
    "\n",
    "score_cols = [agg+'-top-'+str(k)+'-tokens' for agg in agg_methods for k in top_k_tokens] + [agg+'-all-tokens' for agg in agg_methods]\n",
    "line_score_df_col_name = ['commit_id', 'line_level_label'] + score_cols\n",
    "\n",
    "if not os.path.exists('./text_metric_line_eval_result/'):\n",
    "    os.makedirs('/text_metric_line_eval_result/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tmp_df(all_commits,agg_methods):\n",
    "    df = pd.DataFrame(columns = ['commit_id']+agg_methods)\n",
    "    df['commit_id'] = all_commits\n",
    "    df = df.set_index('commit_id')\n",
    "    return df\n",
    "\n",
    "def get_line_level_metrics(line_score,label):\n",
    "    scaler = MinMaxScaler()\n",
    "    line_score = scaler.fit_transform(np.array(line_score).reshape(-1, 1)) # cannot pass line_score as list T-T\n",
    "    pred = np.round(line_score)\n",
    "    \n",
    "    line_df = pd.DataFrame()\n",
    "    line_df['scr'] = [float(val) for val in list(line_score)]\n",
    "    line_df['label'] = label\n",
    "    line_df = line_df.sort_values(by='scr',ascending=False)\n",
    "    line_df['row'] = np.arange(1, len(line_df)+1)\n",
    "\n",
    "    real_buggy_lines = line_df[line_df['label'] == 1]\n",
    "    \n",
    "    p, r, f1, _ = precision_recall_fscore_support(label, pred, average='binary')\n",
    "    auc = roc_auc_score(label, line_score)\n",
    "    mcc = matthews_corrcoef(label, pred)\n",
    "    bal_acc = balanced_accuracy_score(label, pred)\n",
    "    \n",
    "    top_10_acc = 0\n",
    "    \n",
    "    if len(real_buggy_lines) < 1:\n",
    "        IFA = len(line_df)\n",
    "        top_20_percent_LOC_recall = 0\n",
    "        effort_at_20_percent_LOC_recall = math.ceil(0.2*len(line_df))\n",
    "        \n",
    "    else:\n",
    "        IFA = line_df[line_df['label'] == 1].iloc[0]['row']-1\n",
    "        label_list = list(line_df['label'])\n",
    "\n",
    "        all_rows = len(label_list)\n",
    "        \n",
    "        # find top-10 accuracy\n",
    "        if all_rows < 10:\n",
    "            top_10_acc = np.sum(label_list[:all_rows])/len(label_list[:all_rows])\n",
    "        else:\n",
    "            top_10_acc = np.sum(label_list[:10])/len(label_list[:10])\n",
    "\n",
    "        # find recall\n",
    "        LOC_20_percent = line_df.head(int(0.2*len(line_df)))\n",
    "        buggy_line_num = LOC_20_percent[LOC_20_percent['label'] == 1]\n",
    "        top_20_percent_LOC_recall = float(len(buggy_line_num))/float(len(real_buggy_lines))\n",
    "\n",
    "        # find effort @20% LOC recall\n",
    "\n",
    "        buggy_20_percent = real_buggy_lines.head(math.ceil(0.2 * len(real_buggy_lines)))\n",
    "        buggy_20_percent_row_num = buggy_20_percent.iloc[-1]['row']\n",
    "        effort_at_20_percent_LOC_recall = int(buggy_20_percent_row_num) / float(len(line_df))\n",
    "\n",
    "    return IFA, top_20_percent_LOC_recall, effort_at_20_percent_LOC_recall, top_10_acc, p, r, f1, auc, mcc, bal_acc\n",
    "\n",
    "def eval_line_level(cur_proj):\n",
    "    RF_result = pd.read_csv(RF_data_path+cur_proj+'_line_level.csv')\n",
    "    ngram_result = pd.read_csv(ngram_data_path+cur_proj+'_DE_SMOTE_ngram_score.txt',sep='\\t')\n",
    "\n",
    "    RF_result = RF_result[line_score_df_col_name]\n",
    "\n",
    "    all_commits = list(RF_result['commit_id'].unique())\n",
    "\n",
    "    IFA_df = create_tmp_df(all_commits, score_cols+['ngram'])\n",
    "    recall_20_percent_effort_df = create_tmp_df(all_commits, score_cols+['ngram']) \n",
    "    effort_20_percent_recall_df = create_tmp_df(all_commits, score_cols+['ngram'])\n",
    "    precision_df = create_tmp_df(all_commits, score_cols+['ngram'])\n",
    "    recall_df = create_tmp_df(all_commits, score_cols+['ngram'])\n",
    "    f1_df = create_tmp_df(all_commits, score_cols+['ngram'])\n",
    "    AUC_df = create_tmp_df(all_commits, score_cols+['ngram'])\n",
    "    top_10_acc_df = create_tmp_df(all_commits, score_cols+['ngram'])\n",
    "    MCC_df = create_tmp_df(all_commits, score_cols+['ngram'])\n",
    "    bal_ACC_df = create_tmp_df(all_commits, score_cols+['ngram'])\n",
    "\n",
    "    for commit in all_commits:\n",
    "        IFA_list = []\n",
    "        recall_20_percent_effort_list = []\n",
    "        effort_20_percent_recall_list = []\n",
    "        precision_list = []\n",
    "        recall_list = []\n",
    "        f1_list = []\n",
    "        AUC_list = []\n",
    "        top_10_acc_list = []\n",
    "        all_MCC_list = []\n",
    "        all_bal_ACC_df_list = []\n",
    "\n",
    "        cur_RF_result = RF_result[RF_result['commit_id']==commit]\n",
    "\n",
    "        line_label = list(cur_RF_result['line_level_label'])\n",
    "        cur_ngram_result = ngram_result[ngram_result['File-Name']==commit]\n",
    "\n",
    "        for n, agg_method in enumerate(score_cols):\n",
    "            \n",
    "            RF_line_scr = list(cur_RF_result[agg_method])\n",
    "            \n",
    "            IFA, top_20_percent_LOC_recall, effort_at_20_percent_LOC_recall, top_10_acc, p, r, f1, auc, mcc, bal_acc = get_line_level_metrics(RF_line_scr, line_label)\n",
    "\n",
    "            IFA_list.append(IFA)\n",
    "            recall_20_percent_effort_list.append(top_20_percent_LOC_recall)\n",
    "            effort_20_percent_recall_list.append(effort_at_20_percent_LOC_recall)\n",
    "            precision_list.append(p)\n",
    "            recall_list.append(r)\n",
    "            f1_list.append(f1)\n",
    "            AUC_list.append(auc)\n",
    "            top_10_acc_list.append(top_10_acc)\n",
    "            all_MCC_list.append(mcc)\n",
    "            all_bal_ACC_df_list.append(bal_acc)\n",
    "\n",
    "        ngram_line_scr = list(cur_ngram_result['N-Gram-Score'])\n",
    "\n",
    "        IFA, top_20_percent_LOC_recall, effort_at_20_percent_LOC_recall, top_10_acc, p, r, f1, auc, mcc, bal_acc = get_line_level_metrics(ngram_line_scr, line_label)\n",
    "\n",
    "        IFA_list.append(IFA)\n",
    "        recall_20_percent_effort_list.append(top_20_percent_LOC_recall)\n",
    "        effort_20_percent_recall_list.append(effort_at_20_percent_LOC_recall)\n",
    "        precision_list.append(p)\n",
    "        recall_list.append(r)\n",
    "        f1_list.append(f1)\n",
    "        AUC_list.append(auc)\n",
    "        top_10_acc_list.append(top_10_acc)\n",
    "        all_MCC_list.append(mcc)\n",
    "        all_bal_ACC_df_list.append(bal_acc)\n",
    "\n",
    "        IFA_df.loc[commit] = IFA_list\n",
    "        recall_20_percent_effort_df.loc[commit] = recall_20_percent_effort_list\n",
    "        effort_20_percent_recall_df.loc[commit] = effort_20_percent_recall_list\n",
    "        precision_df.loc[commit] = precision_list\n",
    "        recall_df.loc[commit] = recall_list\n",
    "        f1_df.loc[commit] = f1_list\n",
    "        AUC_df.loc[commit] = AUC_list\n",
    "        top_10_acc_df.loc[commit] = top_10_acc_list\n",
    "        MCC_df.loc[commit] = all_MCC_list\n",
    "        bal_ACC_df.loc[commit] = all_bal_ACC_df_list\n",
    "\n",
    "    IFA_df.to_csv('./text_metric_line_eval_result/'+cur_proj+'_IFA.csv',index=False)\n",
    "    recall_20_percent_effort_df.to_csv('./text_metric_line_eval_result/'+cur_proj+'_recall_20_percent_effort.csv',index=False) \n",
    "    effort_20_percent_recall_df.to_csv('./text_metric_line_eval_result/'+cur_proj+'_effort_20_percent_recall.csv',index=False)\n",
    "    precision_df.to_csv('./text_metric_line_eval_result/'+cur_proj+'_precision.csv',index=False)\n",
    "    recall_df.to_csv('./text_metric_line_eval_result/'+cur_proj+'_recall.csv',index=False)\n",
    "    f1_df.to_csv('./text_metric_line_eval_result/'+cur_proj+'_f1.csv',index=False)\n",
    "    AUC_df.to_csv('./text_metric_line_eval_result/'+cur_proj+'_AUC.csv',index=False)\n",
    "    top_10_acc_df.to_csv('./text_metric_line_eval_result/'+cur_proj+'_top_10_acc.csv',index=False)\n",
    "    MCC_df.to_csv('./text_metric_line_eval_result/'+cur_proj+'_MCC.csv',index=False)\n",
    "    bal_ACC_df.to_csv('./text_metric_line_eval_result/'+cur_proj+'_bal_ACC.csv',index=False)\n",
    "    \n",
    "    print('finish', cur_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish qt\n"
     ]
    }
   ],
   "source": [
    "eval_line_level(projects[0])\n",
    "eval_line_level(projects[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Oat)",
   "language": "python",
   "name": "env_oat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
